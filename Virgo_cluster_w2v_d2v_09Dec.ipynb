{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liiS-J479Gvt"
   },
   "source": [
    "# Alternus Vera - Identify Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebnN4ikZzQtu"
   },
   "source": [
    "## Datasets:\n",
    "Original Kaggle fake news dataset: \n",
    "'https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle/kaggle-fake.csv'\n",
    "\n",
    "#### This dataset is heavily skewed to fake news. I moved forward to try to find other dataset that enriches non-fake news.\n",
    "\n",
    "Enriched Kaggle news dataset (50,000 verified non-fake news):\n",
    "https://dock2.hyunwookshin.com/public/cmpe257_a1/articles1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "vymCp8X8IHEk",
    "outputId": "71e6be37-59c8-4f11-c086-eb0564d60a80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_venv/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yuxu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yuxu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/yuxu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yuxu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import chi2\n",
    "from string import punctuation\n",
    "from nltk import PorterStemmer\n",
    "import copy \n",
    "import re, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmVds2371DXz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_venv/lib/python3.6/site-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/anaconda3/envs/nlp_venv/lib/python3.6/site-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "def get_parsed_data2(url):\n",
    "    return pd.read_csv(io.StringIO(requests.get(url, verify=False).content.decode('utf-8')), sep=',', header='infer')\n",
    "\n",
    "# download and parse the dataset...\n",
    "data_kg_fake_news = get_parsed_data2('https://github.com/synle/machine-learning-sample-dataset/raw/master/liar_dataset/kaggle/kaggle-fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmVjSdSC5-DC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_venv/lib/python3.6/site-packages/urllib3/connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "data_kg_nonfake_news = get_parsed_data2('https://dock2.hyunwookshin.com/public/cmpe257_a1/articles1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sq5Yu8r9CuLP"
   },
   "outputs": [],
   "source": [
    "def tokenize2(text):\n",
    "    cachedStopWords = set(stopwords.words('english') + list(punctuation))\n",
    "    min_length = 3\n",
    "    # tokenize\n",
    "    # convert to lower case\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    # steming\n",
    "    tokens = list(map(lambda token: PorterStemmer().stem(token), words))\n",
    "    # lemmatize\n",
    "    lemmas = [WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
    "    # only focus on alphabetic words\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    \n",
    "    filtered_lemmas = list(filter(lambda lemma: p.match(lemma) and len(lemma) >= min_length, lemmas))\n",
    "    return filtered_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQT_pvlW5-DF"
   },
   "outputs": [],
   "source": [
    "data_kg_nonfake_news.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "data_kg_nonfake_news['type'] = 0\n",
    "data_kg_fake_news.loc[data_kg_fake_news['type']!='bs', 'type'] = 0\n",
    "data_kg_fake_news.loc[data_kg_fake_news['type']=='bs', 'type'] = 1\n",
    "all_data = pd.concat([data_kg_fake_news[['title','text','type']], data_kg_nonfake_news[['title','text','type']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqgboZrA5-Da"
   },
   "outputs": [],
   "source": [
    "all_data['text_clean']=all_data['text'].astype('U').apply(tokenize2)\n",
    "all_data['title_clean']=all_data['title'].astype('U').apply(tokenize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    51507\n",
       "1    11492\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(all_data.text_clean, size=50)\n",
    "w2v_trained = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.line_to_vec)\n",
    "    \n",
    "    def line_to_vec(self, line):\n",
    "        row = []\n",
    "        for w in line:\n",
    "            if w not in self.word2vec:\n",
    "                row+=[0]\n",
    "            else:\n",
    "                row+=[np.mean(self.word2vec[w])]\n",
    "        return row  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "embedding = EmbeddingVectorizer(w2v_trained)\n",
    "\n",
    "all_data['text_w2v_mean'] = embedding.transform(all_data['text_clean']).apply(np.mean)\n",
    "all_data['title_w2v_mean'] = embedding.transform(all_data['title_clean']).apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62999 entries, 0 to 49999\n",
      "Data columns (total 7 columns):\n",
      "title             62319 non-null object\n",
      "text              62953 non-null object\n",
      "type              62999 non-null int64\n",
      "text_clean        62999 non-null object\n",
      "title_clean       62999 non-null object\n",
      "text_w2v_mean     62755 non-null float64\n",
      "title_w2v_mean    62784 non-null float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v, [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_text = label_sentences(all_data.text_clean, 'Text')\n",
    "doc2vec_title = label_sentences(all_data.title_clean, 'Title')\n",
    "doc2vec_all_data = doc2vec_text + doc2vec_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125998/125998 [00:00<00:00, 2947181.86it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=50, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(doc2vec_all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125998/125998 [00:00<00:00, 2715302.60it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2733151.19it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2722576.69it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2556533.95it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2867232.27it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2945457.11it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2892925.89it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2933798.45it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2938039.15it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2908016.92it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2752554.33it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2953391.20it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2944915.47it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2915621.64it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2968004.15it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2906657.40it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2948135.44it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2993253.75it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2861271.12it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2764013.83it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2767284.81it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2913258.96it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2948217.68it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2883376.61it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2721160.79it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2977200.42it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2923670.54it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2949402.36it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2942161.08it/s]\n",
      "100%|██████████| 125998/125998 [00:00<00:00, 2891058.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(doc2vec_all_data)]), total_examples=len(doc2vec_all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['text_d2v_mean'] = np.mean(get_vectors(model_dbow, len(all_data.text_clean), 50, 'Text'),axis=1)\n",
    "all_data['title_d2v_mean'] = np.mean(get_vectors(model_dbow, len(all_data.title_clean), 50, 'Title'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "all_data_ = deepcopy(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = all_data_[['text_w2v_mean','title_w2v_mean', 'text_d2v_mean', 'title_d2v_mean']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.to_csv('fake_news_w2v_d2v.csv')\n",
    "all_data[['text_w2v_mean','title_w2v_mean', 'text_d2v_mean', 'title_d2v_mean']].to_csv('fake_news_w2v_d2v_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('fake_news_w2v_d2v_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "YuXu_word2vec_doc2vec_fake_news.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
